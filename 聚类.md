# 聚类模型的性能度量-内部指标（一般用于训练后度量）
## Q：Dunn指数是越大越好吗，轮廓系数和DB指数呢
是的，下面是这三个聚类评价指标的**含义与优化方向总结**：

---

## ✅ 1. **Dunn 指数（Dunn Index）**

- **定义**：最小簇间距离 / 最大簇内直径
    
- **目标**：希望簇与簇之间距离大（分得开），同簇内部距离小（聚得紧）
    

Dunn 指数=min⁡i≠jdist(Ci,Cj)max⁡kdiam(Ck)\text{Dunn 指数} = \frac{\min\limits_{i \ne j} \text{dist}(C_i, C_j)}{\max\limits_{k} \text{diam}(C_k)}

- **解释**：
    
    - 分子大 → 簇间距大
        
    - 分母小 → 簇内部紧密
        
- ✅ **越大越好**
    
# 原型聚类
![[Snipaste_2025-05-01_16-39-09.png]]


好的，我们从**数学角度**来看主流聚类算法的**目标函数（Objective Function）**与其**优化方法（Optimization Method）**。

---


---

## ✅ 2. **轮廓系数（Silhouette Coefficient）**

- **定义**（每个样本）：
    

s(i)=b(i)−a(i)max⁡{a(i),b(i)}s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}

其中：

- a(i)a(i)：样本 ii 到本簇内其它点的平均距离
    
- b(i)b(i)：样本 ii 到最近的其他簇的平均距离
    
- **范围**：−1≤s(i)≤1-1 \le s(i) \le 1
    
- **解释**：
    
    - s(i)→1s(i) \to 1：很好地划分到正确簇
        
    - s(i)→0s(i) \to 0：样本在簇边界
        
    - s(i)→−1s(i) \to -1：划错类了
        
- ✅ **越大越好**
    

---

## ✅ 3. **Davies-Bouldin 指数（DB 指数）**

- **定义**：每个簇与最相似的其他簇之间的“紧凑度 + 分离度”的比值，再取平均：
    

DB=1K∑i=1Kmax⁡j≠i(σi+σjdij)\text{DB} = \frac{1}{K} \sum_{i=1}^{K} \max_{j \ne i} \left( \frac{\sigma_i + \sigma_j}{d_{ij}} \right)

其中：

- σi\sigma_i：第 ii 个簇的平均类内距离
    
- dijd_{ij}：簇 ii 与簇 jj 的中心距离
    
- **解释**：
    
    - 分母大 → 簇间距大
        
    - 分子小 → 簇内紧密
        
- ✅ **越小越好**
    

---

## 🧠 总结表格

|指标名称|趋势目标|解释|
|---|---|---|
|Dunn 指数|越大越好 ✅|簇间距大，簇内距离小|
|轮廓系数 Silhouette|越大越好 ✅|越接近 1，聚类效果越清晰|
|Davies-Bouldin 指数|越小越好 ✅|越小，说明簇间分得越开，越合理|

---

需要我帮你把这几个指标都封装成函数统一输出，或者批量跑多个 KK 值看趋势图吗？

## 🧮 一、K-Means 聚类

### 🎯 目标函数（最小化簇内平方误差）：

$$
设样本集为 {x1,x2,…,xn}⊂Rd\{x_1, x_2, \dots, x_n\} \subset \mathbb{R}^d，要分为 K 个簇，每个簇中心为\mu_k。
$$

目标函数：

$$
J = \sum_{k=1}^{K} \sum_{x_i \in C_k} \|x_i - \mu_k\|^2
$$

即最小化所有样本到其所属簇中心的欧几里得距离平方之和。
其中：
$\boldsymbol{\mu}_k = \frac{1}{|C_k|} \sum_{\mathbf{x}_i \in C_k} \mathbf{x}_i ：第 k 个簇的质心$

[[Snipaste_2025-05-01_17-06-16.png|算法伪代码]]



### ⚙️ 优化方法：

1. 随机初始化 $K$ 个簇中心 $\mu_k$
    
2. E-step：将每个样本分配给最近的中心
    
3. M-step：重新计算每个簇的中心为所有成员的均值
    
4. 重复步骤 2–3 直到收敛（中心不变或迭代次数达到上限）
    

优化方法是**期望-最大化（EM）的一种特例**，但不保证全局最优（受初始点影响）。

---

## 🧮 二、高斯混合模型（GMM）

### 🎯 目标函数：

GMM 假设数据来自 KK 个高斯分布的混合模型，最大化**对数似然函数**：

$$
\log p(X | \Theta) = \sum_{i=1}^{n} \log \left( \sum_{k=1}^{K} \pi_k \cdot \mathcal{N}(x_i | \mu_k, \Sigma_k) \right)
$$

其中：

- $\pi_k 是第 k 个分布的权重，满足\sum \pi_k = 1$
    
- $\mu_k, \Sigma_k 是第 k 个高斯的均值和协方差矩阵$
    

### ⚙️ 优化方法：

使用**期望最大化（EM）算法**：

- **E-step**：计算每个点属于第 kk 类的后验概率（软分配）
    
- **M-step**：根据后验概率更新 $\mu_k, \Sigma_k, \pi_k$
    

直到对数似然收敛。

[[Snipaste_2025-05-01_17-19-21.png|算法伪代码]]

---
### Q：假设数据来自K个高斯分布的混合模型是什么意思，是假设数据分布是若干个不同的高斯分布相加？
是的，你理解得非常对！

---

#### 🎯 “数据来自 KK 个高斯分布的混合模型” 的意思：

就是说我们**假设整个数据集的分布**，不是一个单独的高斯分布，而是由 KK 个不同的高斯分布“加起来”形成的。

这在数学上表示为一个 **高斯混合分布（Gaussian Mixture Model, GMM）**：

$p(x) = \sum_{k=1}^{K} \pi_k \cdot \mathcal{N}(x | \mu_k, \Sigma_k)$

其中：

- $\pi_k$ 是第 k 个高斯分布的**权重系数**，满足：
    
    $\pi_k \ge 0,\quad \sum_{k=1}^{K} \pi_k = 1$
- $\mathcal{N}(x | \mu_k, \Sigma_k)$ 是第 k 个**高斯分布的概率密度函数**，均值是 $\mu_k$，协方差是 $\Sigma_k$
    
- 所以整体是这 K 个高斯分布**按比例相加形成的混合分布**
    

---

#### 🔍 形象类比：

可以把这个模型想象成这样：

- 每次“生成一个数据点”前，先从 KK 个高斯分布中**随机选一个分布**（选中第 kk 个的概率是 πk\pi_k）
    
- 然后**从这个被选中的高斯分布里采样一个点**
    

整个数据集就是大量这种“选择一个高斯 → 采样点”的结果。

---

#### 📊 示例图像（可视化示意）：

如果你将 GMM 应用于二维数据，你会看到数据点在空间中形成多个椭圆形状的“团”，每团近似服从一个高斯分布。

---

#### ✅ GMM 的聚类思想：

- **每个高斯分布表示一个“簇”**
    
- **每个样本点属于所有簇的“概率权重”不同**（这叫**软聚类**，不像 K-means 那样是硬分配）
    
- 聚类过程就是**估计每个样本点来自哪个高斯的概率最大**
    

---

如果你想我可以具体举个 2D 高斯混合分布例子（含图），或者推导它的概率密度公式。你对哪个部分最感兴趣？
