# 并行与分布式计算课程实验报告

## 实验题目：大文本关键词统计的并行化设计与实现

---

## 一、实验目的

本实验旨在设计并实现一个大规模文本关键词统计系统，探索在不同实现方式（串行与并行）下的性能差异。通过引入数据预处理、划分策略、负载均衡、并行计算与可视化等关键技术，理解并掌握在实际数据处理中如何利用并行化手段提高处理效率。

---

## 二、实验环境

- 操作系统：Windows 11  
- 编程语言：Python 3.11  
- 使用库：multiprocessing、matplotlib、wordcloud、collections  
- CPU 规格：22 核逻辑处理器  
- 测试文本：`bigtext_large.txt`，大小约 320MB  
- 实验工具：PyCharm + 虚拟环境

---

## 三、系统功能模块设计

本系统采用模块化设计，主要包含以下六个部分：

1. **preprocess.py**：实现文本预处理，包括去除标点符号、统一大小写、分词和去停用词等操作。
2. **parallel_count.py**：使用 Python 的 `multiprocessing` 并行库对文本进行分段处理，并统计关键词频率。
3. **benchmark.py**：比较串行与并行两种实现方式的耗时差异，并输出加速比。
4. **visualize.py**：使用 `matplotlib` 和 `wordcloud` 生成高频关键词柱状图和词云图。
5. **main.py**：主程序入口，调用其他模块，执行完整流程。
6. **data/bigtext_large.txt**：实验使用的合成大文本文件，约 320MB。

---

## 四、关键技术实现与说明

1. **数据预处理**：通过正则表达式清除标点、统一大小写，并对停用词进行过滤，保留有效关键词以提高统计准确性。
2. **划分策略**：将大文本按行平均划分为多个子块，每个子块分配给一个工作进程，确保任务负载均衡。
3. **并行调度**：采用 `multiprocessing.Pool` 对子任务进行并发处理，每个进程返回一个词频字典（`Counter`），最终由主进程合并所有结果。
4. **性能优化**：使用 Python 标准库中的高效数据结构 `collections.Counter` 进行词频累加操作，并在预处理阶段尽量避免重复开销。
5. **可视化展示**：输出高频关键词柱状图和词云图，直观展示文本主题及关键词分布情况。

---

## 五、实验数据与结果分析

在本次实验中，我们使用大小约为 320MB 的纯文本文件进行统计测试。通过比较串行与并行处理同一文件的耗时，得到以下结果：

- 串行处理耗时：`24.95 秒`  
- 并行处理耗时：`7.95 秒`  
- 加速比 ≈ `3.14 倍`

结果表明，在中等规模数据下，采用合理的并行划分策略可以显著减少总运行时间，提升处理效率。

此外，关键词统计结果符合语料内容主题。例如：

- 高频人物名如 `pierre`, `prince`
- 高频动词如 `said`, `were`, `did`
- 高频结构词如 `which`, `when`, `who`

说明系统统计结果具有正确性与代表性。

---

## 六、实验结论

本实验成功实现了一个模块化的并行关键词统计系统。在测试过程中，使用多进程技术有效地提高了处理速度，达到了 3 倍以上的加速效果。实验验证了如下结论：

1. 并行计算在大数据文本处理任务中具有显著优势；
2. 合理划分策略和负载均衡设计是系统性能优化的关键；
3. 可视化输出有助于从数据中快速获取有效信息；
4. 模块化设计有助于系统拓展与调试。

---

## 七、改进与拓展方向

1. 使用 PySpark 实现分布式版本，处理超大规模文本数据；
2. 增加动态负载调度机制，进一步优化多进程效率；
3. 支持多种划分策略（如按词哈希、按段落划分）进行对比；
4. 添加内存与 CPU 占用统计，实现更全面的性能分析；
5. 将统计结果写入 CSV 并自动生成分析报告图表。

---

## 八、附录（可视化与结果截图）


- 实验运行截图（代码运行日志与 Top 30 词频结果）
- Top 30 关键词柱状图
- 词云图
-  源代码及源码目录结构

```
附件/code/大文本关键词统计的并行化设计与实现code/
├── data/
│   ├── bigtext_large.txt                  # 合成的大文本文件，用于词频统计
│   └── create_bigdata_by_copy_self.py     # 文本复制生成工具脚本
├── benchmark.py                            # 串行 vs 并行统计耗时对比模块
├── main.py                                 # 程序入口，组织各模块运行流程
├── parallel_count.py                       # 使用 multiprocessing 实现关键词并行统计
├── preprocess.py                           # 数据清洗与预处理：分词、去除停用词等
└── visualize.py                            # 使用 matplotlib 和 wordcloud 进行可视化
```

 快速访问链接
- [main.py](附件/code/大文本关键词统计的并行化设计与实现code/main.py)
- [benchmark.py](大文本关键词统计的并行化设计与实现code/benchmark.py)
- [parallel_count.py](大文本关键词统计的并行化设计与实现code/parallel_count.py)
- [preprocess.py](大文本关键词统计的并行化设计与实现code/preprocess.py)
- [visualize.py](大文本关键词统计的并行化设计与实现code/visualize.py)
- [create_bigdata_by_copy_self.py](大文本关键词统计的并行化设计与实现code/data/create_bigdata_by_copy_self.py)
- [bigtext_large.txt](大文本关键词统计的并行化设计与实现code/data/bigtext_large.txt)